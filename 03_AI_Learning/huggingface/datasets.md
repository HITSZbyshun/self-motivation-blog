# datasets



## load_dataset

``` python
from datasets import load_dataset
```



- æ•°æ®é›†å­˜å‚¨åœ¨å„ç§ä½ç½®ï¼Œæ¯”å¦‚ 
  - Hub 
  - æœ¬åœ°è®¡ç®—æœºçš„ç£ç›˜ä¸Š
  - Github å­˜å‚¨åº“ä¸­
  - å†…å­˜ä¸­çš„æ•°æ®ç»“æ„ï¼ˆå¦‚ Python è¯å…¸å’Œ Pandas DataFramesï¼‰ä¸­
- æ— è®ºæ‚¨çš„æ•°æ®é›†å­˜å‚¨åœ¨ä½•å¤„ï¼ŒğŸ¤— Datasets éƒ½ä¸ºæ‚¨æä¾›äº†ä¸€ç§åŠ è½½å’Œä½¿ç”¨å®ƒè¿›è¡Œè®­ç»ƒçš„æ–¹æ³•ã€‚



### hugging face hub

ä½¿ç”¨`datasets.load_dataset()`åŠ è½½Hubä¸Šçš„æ•°æ®é›†ã€‚



```python
from datasets import load_dataset
data_files = {"train": "train.csv", "test": "test.csv"}

dataset = load_dataset('lhoestq/demo1',
                       revision="main",
                       data_files=data_files,
                       data_files='en/c4-train.0000*-of-01024.json.gz')
```

- å‚æ•°æ˜¯å­˜å‚¨åº“å‘½åç©ºé—´å’Œæ•°æ®é›†åç§°ï¼ˆepository mespace and dataset nameï¼‰
- æ ¹æ®revisionåŠ è½½æŒ‡å®šç‰ˆæœ¬æ•°æ®é›†ï¼šï¼ˆæŸäº›æ•°æ®é›†å¯èƒ½æœ‰Git æ ‡ç­¾ã€branches or commitså¤šä¸ªç‰ˆæœ¬ï¼‰
- ä½¿ç”¨è¯¥data_fileså‚æ•°å°†æ•°æ®æ–‡ä»¶æ˜ å°„åˆ°æ‹†åˆ†ï¼Œä¾‹å¦‚train,validationå’Œtestï¼š(å¦‚æœæ•°æ®é›†æ²¡æœ‰æ•°æ®é›†åŠ è½½è„šæœ¬ï¼Œåˆ™é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ•°æ®éƒ½å°†åœ¨trainæ‹†åˆ†ä¸­åŠ è½½ã€‚)
- ä½¿ç”¨data_fileså‚æ•°åŠ è½½æ–‡ä»¶çš„ç‰¹å®šå­é›†



### æœ¬åœ°å’Œè¿œç¨‹æ–‡ä»¶

æœ¬åœ°æˆ–è¿œç¨‹çš„æ•°æ®é›†ï¼Œå­˜å‚¨ç±»å‹ä¸ºcsvï¼Œjsonï¼Œtxtæˆ–parquetæ–‡ä»¶éƒ½å¯ä»¥åŠ è½½



#### CSV

```python
#å¤šä¸ª CSV æ–‡ä»¶ï¼š
dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])

#å°†è®­ç»ƒå’Œæµ‹è¯•æ‹†åˆ†æ˜ å°„åˆ°ç‰¹å®šçš„ CSV æ–‡ä»¶ï¼š
dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'] 'test': 'my_test_file.csv'})

#è¦é€šè¿‡ HTTP åŠ è½½è¿œç¨‹ CSV æ–‡ä»¶ï¼Œæ‚¨å¯ä»¥ä¼ é€’ URLï¼š
base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"

dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})
```



#### JSON

```python
from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json')
```





#### å¯¼å‡º

| æ–‡ä»¶ç±»å‹             | å¯¼å‡ºæ–¹å¼                                                     |
| -------------------- | ------------------------------------------------------------ |
| CSV                  | datasets.Dataset.to_csv()                                    |
| json                 | datasets.Dataset.to_json()                                   |
| Parquet              | datasets.Dataset.to_parquet()                                |
| å†…å­˜ä¸­çš„ Python å¯¹è±¡ | datasets.Dataset.to_pandas() æˆ–è€… datasets.Dataset.to_dict() |




## load_from_disk

ä¿å­˜å’ŒåŠ è½½dataset

``` python
encoded_dataset.save_to_disk("path/of/my/dataset/directory")

from datasets import load_from_disk
reloaded_encoded_dataset = load_from_disk("path/of/my/dataset/directory")
```







## æœåŠ¡å™¨æ— vpnæ•°æ®é›†å¦‚ä½•è·å–ï¼Ÿ

### æ–¹æ³•ä¸€

å°æ•°æ®é›†åœ¨æœ¬åœ°ä¸‹è½½ï¼Œç„¶åä¼ è¾“åˆ°æœåŠ¡å™¨

```python
from datasets import Dataset, load_dataset, load_from_disk
dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')
dataset.save_to_disk("/media/theo/c8f79183-f83c-4c2e-8065-ce6580d2a20e/datasets/wikitext") # ä¿å­˜åˆ°è¯¥ç›®å½•ä¸‹
dataset
```

