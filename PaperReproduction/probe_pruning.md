# probe_pruning

{'asyncintra_on_diff_gpu': False, 'batch_size': 20, 'calib_info': 'c4-2000', 'calibration_dataset': 'c4', 'calibration_nsamples': 2000, 'calibration_stage': False, 'collate_mode': 'transformer', 'control': {'batch_size': '20', 'calib_info': 'c4-2000', 'cust_tgt_modules': 'default', 'data_name': 'wikitext-2v1', 'max_seq_len': '1024', 'mode': 'sync', 'model_name': 'llama-2-7b', 'prune_info': '0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-seqrank+bszrank', 'prune_method': 'probe-default', 'prune_metric': 'ppwandasp', 'prune_ratio': '0.4', 'task_name': 'clm'}, 'control_name': 'wikitext-2v1_llama-2-7b_clm_20_1024_0.4_ppwandasp_probe-default_sync_c4-2000_0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-seqrank+bszrank_default', 'cudatoolkit_version': 11.8, 'cudnn_version': 8700.0, 'cur_batch_index': -1, 'cust_tgt_modules': ['q_proj', 'v_proj', 'o_proj', 'k_proj', 'gate_proj', 'up_proj', 'down_proj'], 'custom_cuda_streams': {0: <torch.cuda.Stream device=cuda:0 cuda_stream=0xa5a9030>}, 'data_name': 'wikitext', 'data_type': torch.float16, 'data_type_max': 65504.0, 'data_type_min': -65504.0, 'default_cuda_streams': {0: <torch.cuda.Stream device=cuda:0 cuda_stream=0x0>}, 'device': 'cuda', 'ema_momentum': 0.99, 'epoch': 0, 'gate_probe_ratio': [0.5, 0.05], 'gate_prune': '0.5+0.05', 'gpt2': {'max_length': 1024}, 'gpu_name': 'NVIDIA GeForce RTX 3090', 'hf_data_name': 'wikitext', 'hf_subset_name': 'wikitext-2-raw-v1', 'init_seed': 0, 'k_probe_ratio': [0.5, 0.05], 'k_prune': '0.5+0.05', 'label_column': None, 'llama-2-7b': {'batch_size': {'test': 20, 'train': 20}, 'max_length': 1024, 'shuffle': {'test': False, 'train': False}}, 'log_interval': 0.25, 'logger_detailed_info': False, 'max_seq_len': 1024, 'mode': 'sync', 'model_name': 'llama-2-7b', 'model_tag': '0_wikitext-2v1_llama-2-7b_clm_20_1024_0.4_ppwandasp_probe-default_sync_c4-2000_0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-seqrank+bszrank_default', 'num_experiments': 1, 'num_workers': 0, 'onlyprobe': False, 'onlyprobeinfo': False, 'pad_tokens': None, 'pin_memory': True, 'probe_generation_type': ['seqrank', 'bszrank'], 'prune_info': '0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-seqrank+bszrank', 'prune_method': 'probe-default-calib-ema-respick', 'prune_metric': 'ppwandasp', 'prune_ratio': 0.4, 'q_probe_ratio': [0.5, 0.05], 'q_prune': '0.5+0.05', 'qk_prune_way': 'whole', 'resume_mode': 0, 'seed': 0, 'skip_layers': [0, 1, 2], 'subset_name': '2v1', 'task_name': 'clm', 'tc_multiple': 64, 'test_speed': False, 'text_column': ['text'], 'up_probe_ratio': [0.5, 0.05], 'up_prune': '0.5+0.05', 'v_probe_ratio': [0.5, 0.05], 'v_prune': '0.5+0.05', 'verbose': False, 'vo_prune_way': 'whole', 'world_size': 1}





{'asyncintra_on_diff_gpu': False, 'batch_size': 20, 'cache_model_path': 'output/model/llama-2-7b', 'cache_tokenizer_path': 'output/tokenizer/llama-2-7b', 'calib_info': 'c4-2000', 'calibration_dataset': 'c4', 'calibration_nsamples': 2000, 'calibration_stage': False, 'collate_mode': 'transformer', 'control': {'batch_size': '20', 'calib_info': 'c4-2000', 'cust_tgt_modules': 'default', 'data_name': 'wikitext-2v1', 'max_seq_len': '1024', 'mode': 'sync', 'model_name': 'llama-2-7b', 'prune_info': '0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-seqrank+bszrank', 'prune_method': 'probe-default', 'prune_metric': 'ppwandasp', 'prune_ratio': '0.4', 'task_name': 'clm'}, 'control_name': 'wikitext-2v1_llama-2-7b_clm_20_1024_0.4_ppwandasp_probe-default_sync_c4-2000_0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-seqrank+bszrank_default', 'cudatoolkit_version': 11.8, 'cudnn_version': 8700.0, 'cur_batch_index': -1, 'cust_tgt_modules': ['q_proj', 'v_proj', 'o_proj', 'k_proj', 'gate_proj', 'up_proj', 'down_proj'], 'custom_cuda_streams': {0: <torch.cuda.Stream device=cuda:0 cuda_stream=0xa4e3150>}, 'data_name': 'wikitext', 'data_type': torch.float16, 'data_type_max': 65504.0, 'data_type_min': -65504.0, 'default_cuda_streams': {0: <torch.cuda.Stream device=cuda:0 cuda_stream=0x0>}, 'device': 'cuda', 'ema_momentum': 0.99, 'epoch': 0, 'gate_probe_ratio': [0.5, 0.05], 'gate_prune': '0.5+0.05', 'gpt2': {'max_length': 1024}, 'gpu_name': 'NVIDIA GeForce RTX 3090', 'hf_data_name': 'wikitext', 'hf_subset_name': 'wikitext-2-raw-v1', 'init_seed': 0, 'k_probe_ratio': [0.5, 0.05], 'k_prune': '0.5+0.05', 'label_column': None, 'llama-2-7b': {'batch_size': {'test': 20, 'train': 20}, 'max_length': 1024, 'shuffle': {'test': False, 'train': False}}, 'log_interval': 0.25, 'logger_detailed_info': False, 'max_seq_len': 1024, 'mode': 'sync', 'model_name': 'llama-2-7b', 'model_name_or_path': 'output/llama-2-7b', 'model_tag': '0_wikitext-2v1_llama-2-7b_clm_20_1024_0.4_ppwandasp_probe-default_sync_c4-2000_0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-seqrank+bszrank_default', 'num_experiments': 1, 'num_workers': 0, 'onlyprobe': False, 'onlyprobeinfo': False, 'pad_tokens': None, 'pin_memory': True, 'probe_generation_type': ['seqrank', 'bszrank'], 'prune_info': '0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-0.5+0.05-seqrank+bszrank', 'prune_method': 'probe-default-calib-ema-respick', 'prune_metric': 'ppwandasp', 'prune_ratio': 0.4, 'q_probe_ratio': [0.5, 0.05], 'q_prune': '0.5+0.05', 'qk_prune_way': 'whole', 'resume_mode': 0, 'seed': 0, 'skip_layers': [0, 1, 2], 'subset_name': '2v1', 'task_name': 'clm', 'tc_multiple': 64, 'test_speed': False, 'text_column': ['text'], 'tokenizer_name_or_path': 'output/llama-2-7b', 'up_probe_ratio': [0.5, 0.05], 'up_prune': '0.5+0.05', 'v_probe_ratio': [0.5, 0.05], 'v_prune': '0.5+0.05', 'verbose': False, 'vo_prune_way': 'whole', 'world_size': 1}



output/llama-2-7b

output/model/llama-2-7b